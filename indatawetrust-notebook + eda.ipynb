{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport re\nimport joblib\nimport operator\nimport string\nfrom collections import defaultdict\nfrom tqdm import tqdm\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings #to modify warnings\nwarnings.filterwarnings('ignore') #ignoring warnings\nsns.set_style('darkgrid')\n\nfrom wordcloud import STOPWORDS\n\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, recall_score, precision_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/penyisihan-datavidia-7-0/train.csv')\ntest = pd.read_csv('/kaggle/input/penyisihan-datavidia-7-0/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(data, var):\n    corpus=[]\n    \n    for x in data[var].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n\ndef get_data_contains_word(data, feature, string) :\n    _id = []\n    for text,ID in zip(data[feature], data['review_id']) :\n        if string in text :\n            _id.append(ID)\n    return data[data['review_id'].isin(_id)]\n\ndef remove_indonesian_stop(data, feature) :\n    stopwords = fix_stopwords\n    filtered = []\n    for text in data[feature] :\n        text_list = []\n        for word in text.split() :\n            if word not in stopwords :\n                text_list.append(word)\n        text_list = ' '.join(text_list)\n        filtered.append(text_list)\n    return filtered\n\ndef docs_contain_word_plot(data, var, num_words, width, height) :\n    # data berupa df\n    words = []\n    counts = []\n    corpus = create_corpus(data, var)\n    corpus = list(set(corpus))\n    for word in tqdm(corpus) :\n        words.append(word)\n        counts.append(get_data_contains_word(data, var, word).shape[0])\n    \n    res = pd.DataFrame({'words' : words,\n                        'counts' : counts})\n    res = res.sort_values(by = 'counts', ascending = False)\n    plt.figure(figsize = [width, height])\n    sns.barplot(data = res.iloc[:num_words],\n                x = 'counts',\n                y = 'words')\n    for index, value in enumerate(res['counts'].iloc[:num_words]) :\n        plt.text(value, index, value)\n    plt.show()\n    \ndef num_count(data, feature) :\n    #kompas_list = list(set(kompas['words']))\n    num_list = []\n    for sentence in tqdm(data[feature]) :\n        num = 0\n        for word in sentence :\n            if word.isdigit() :\n                num += 1\n            else :\n                num += 0\n        num_list.append(num)\n    return num_list\n\ndef plot_unique_word_count(corpus, width, height, range1, range2, title, color, ax = None) :\n    words = []\n    values = []\n    len_words = []\n    for word, value in zip(pd.DataFrame(corpus).value_counts().index, pd.DataFrame(corpus).value_counts()) :\n        words.append(word[0])\n        values.append(value)\n        len_words.append(len(word[0]))\n    res = pd.DataFrame({'words' : words,\n                        'values' : values,\n                        'len_words' : len_words})\n    res = res.sort_values(by = 'values', ascending = False)\n    #plt.figure(figsize = [width, height])\n    ax = ax\n    ax.set_title(title)\n    sns.barplot(data = res[range1:range2],\n                y = 'words',\n                x = 'values',\n                color = color,\n                ax = ax)\n    for index, value in enumerate(res['values'].iloc[range1:range2]) :\n        plt.text(value, index, value)\n    #plt.show()\n    \n    return ax\n\ndef df_unique_word_count(corpus) :\n    words = []\n    values = []\n    len_words = []\n    for word, value in zip(pd.DataFrame(corpus).value_counts().index, pd.DataFrame(corpus).value_counts()) :\n        words.append(word[0])\n        values.append(value)\n        len_words.append(len(word[0]))\n    res = pd.DataFrame({'words' : words,\n                        'values' : values,\n                        'len_words' : len_words})\n    res = res.sort_values(by = 'values', ascending = False)\n    return res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### INDONESIAN STOPWORDS"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = ['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua', 'ia', 'seperti', 'jika', 'jika', 'sehingga', 'kembali', 'dan', 'ini', 'karena', 'kepada', 'oleh', 'saat', 'harus', 'setelah', 'kami', 'sekitar', 'bagi', 'serta', 'di', 'dari', 'telah', 'sebagai', 'masih', 'hal', 'ketika', 'adalah', 'itu', 'dalam', 'bisa', 'bahwa', 'atau', 'hanya', 'kita', 'dengan', 'akan', 'juga', 'ada', 'mereka', 'sudah', 'saya', 'terhadap', 'secara', 'agar', 'lain', 'anda', 'begitu', 'mengapa', 'kenapa', 'yaitu', 'yakni', 'daripada', 'itulah', 'lagi', 'maka', 'tentang', 'demi', 'dimana', 'kemana', 'pula', 'sambil', 'sebelum', 'sesudah', 'supaya', 'guna', 'kah', 'pun', 'sampai', 'sedangkan', 'selagi', 'sementara', 'apakah', 'sebab', 'selain', 'seolah', 'seraya', 'seterusnya', 'tanpa', 'agak', 'boleh', 'dapat', 'dsb', 'dst', 'dll', 'dahulu', 'dulunya', 'anu', 'demikian', 'ingin', 'juga', 'nggak', 'mari', 'nanti', 'melainkan', 'oh', 'ok', 'seharusnya', 'sebetulnya', 'setiap', 'setidaknya', 'sesuatu', 'pasti', 'saja', 'toh', 'ya', 'walau', 'tolong', 'tentu', 'amat', 'apalagi', 'bagaimanapun', 'sekali', 'jadi', 'nya']\nkeep_stopwords = ['tidak', 'sementara', 'belum', 'tetapi', 'kecuali', 'tapi', 'ada', 'tanpa', 'nggak', 'ok', 'hanya', 'kurang']\nfix_stopwords = [next if word in keep_stopwords else word for word in stopwords]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Typo Words Dictionary"},{"metadata":{"trusted":true},"cell_type":"code","source":"words_dict = {\n    'tdk'  : 'tidak',\n    'yg' : 'yang',\n    'ga' : 'tidak',\n    'gak' : 'tidak',\n    'tp' : 'tapi',\n    'd' : 'di',\n    'sy' : 'saya',\n    '&' : 'dan',\n    'dgn' : 'dengan', \n    'utk' : 'untuk',\n    'gk' : 'tidak',\n    'jd' : 'jadi',\n    'jg' : 'juga',\n    'dr' : 'dari',\n    'krn' : 'karena',\n    'aja' : 'saja',\n    'karna' : 'karena',\n    'udah' : 'sudah',\n    'kmr' : 'kamar',\n    'g' : 'tidak',\n    'dpt' : 'dapat',\n    'banget' : 'sekali',\n    'bgt' : 'sekali',\n    'kalo' : 'kalau',\n    'n' : 'dan', \n    'bs' : 'bisa',\n    'oke' : 'ok',\n    'dg' : 'dengan',\n    'pake' : 'pakai',\n    'sampe' : 'sampai',\n    'dapet' : 'dapat',\n    'ad' : 'ada',\n    'lg' : 'lagi',\n    'bikin' : 'buat',\n    'tak' : 'tidak',\n    'ny' : 'nya',\n    'ngga' : 'tidak',\n    'nunggu' : 'tunggu',\n    'klo' : 'kalau',\n    'blm' : 'belum',\n    'trus' : 'terus',\n    'kayak' : 'seperti',\n    'dlm' : 'dalam',\n    'udh' : 'sudah',\n    'tau' : 'tahu',\n    'org' : 'orang',\n    'hrs' : 'harus',\n    'msh' : 'masih',\n    'sm' : 'sama',\n    'byk' : 'banyak',\n    'krg' : 'kurang',\n    'kmar' : 'kamar',\n    'spt' : 'seperti',\n    'pdhl' : 'padahal',\n    'chek' : 'cek',\n    'pesen' : 'pesan', \n    'kran' : 'keran',\n    'gitu' : 'begitu',\n    'tpi' : 'tapi',\n    'lbh' : 'lebih',\n    'tmpt' : 'tempat',\n    'dikasi' : 'dikasih',\n    'serem' : 'seram', \n    'sya' : 'saya',\n    'jgn' : 'jangan',\n    'dri' : 'dari',\n    'dtg' : 'datang',\n    'gada' : 'tidak ada',\n    'standart' : 'standar',\n    'mlm' : 'malam',\n    'k'  : 'ke',\n    'kl' : 'kalau',\n    'sgt': 'sangat',\n    'y' : 'ya',\n    'krna' : 'karena',\n    'tgl' : 'tanggal', \n    'terimakasih' : 'terima kasih',\n    'kecoak' : 'kecoa',\n    'pd' : 'pada',\n    'tdr' : 'tidur', \n    'jdi' : 'jadi',\n    'kyk' : 'seperti',\n    'sdh' : 'sudah',\n    'ama' : 'sama',\n    'gmana' : 'bagaimana',\n    'dalem' : 'dalam',\n    'tanyak' : 'tanya',\n    'taru' : 'taruh',\n    'gede' : 'besar',\n    'kaya' : 'seperti',\n    'access' : 'akses',\n    'tetep' : 'tetap',\n    'mgkin' : 'mungkin',\n    'sower' : 'shower',\n    'idup' : 'hidup',\n    'nyaaa' : 'nya',\n    'baikk' : 'baik',\n    'hanay' : 'hanya',\n    'tlp' : 'telpon',\n    'kluarga' : 'keluarga',\n    'jln' : 'jalan',\n    'hr' : 'hari',\n    'ngak' : 'tidak',\n    'bli' : 'beli',\n    'kmar' : 'kamar',\n    'naro' : 'taruh'\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['review_text_cleaned'] = [i.lower() for i in train['review_text']]\ntest['review_text_cleaned'] = [i.lower() for i in test['review_text']]\n\n# removing \\x00 characters\ntrain['review_text_cleaned'] = [re.sub(r'[^\\x00-\\x7f]',r'', i) for i in train['review_text_cleaned']]\ntest['review_text_cleaned'] = [re.sub(r'[^\\x00-\\x7f]',r'', i) for i in test['review_text_cleaned']]\n\n#r removing \\n\ntrain['review_text_cleaned'] = [re.sub(r'\\n', r' ', i) for i in train['review_text_cleaned']]\ntest['review_text_cleaned'] = [re.sub(r'\\n', r' ', i) for i in test['review_text_cleaned']]\n\n#r removing numbers\ntrain['review_text_cleaned'] = [re.sub(r\"\\d+\", r\"\", i) for i in train['review_text_cleaned']]\ntest['review_text_cleaned'] = [re.sub(r\"\\d+\", r\"\", i) for i in test['review_text_cleaned']]\n\n#removing punctuation\ntrain['review_text_cleaned'] = [i.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) for i in train['review_text_cleaned']]\ntest['review_text_cleaned'] = [i.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) for i in test['review_text_cleaned']]\n\ndef num_count(data, feature) :\n    #kompas_list = list(set(kompas['words']))\n    num_list = []\n    for sentence in tqdm(data[feature]) :\n        num = 0\n        for word in sentence :\n            if word.isdigit() :\n                num += 1\n            else :\n                num += 0\n        num_list.append(num)\n    return num_list\n\ntrain['num_of_space'] = [i.count(' ') for i in train['review_text_cleaned']]\ntest['num_of_space'] = [i.count(' ') for i in test['review_text_cleaned']]\n\ntrain['num_of_words'] = [len(i.split()) for i in train['review_text_cleaned']]\ntest['num_of_words'] = [len(i.split()) for i in test['review_text_cleaned']]\n\ntrain['num_unique_char'] = [len(set(i)) for i in train['review_text_cleaned']]\ntest['num_unique_char'] = [len(set(i)) for i in test['review_text_cleaned']]\n\ntrain['len_string_initial'] = [len(i) for i in train['review_text']]\ntest['len_string_initial'] = [len(i) for i in test['review_text']]\n\ntrain['num_of_numeric'] = num_count(train, 'review_text')\ntest['num_of_numeric'] = num_count(test, 'review_text')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### initial cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_sentence_train = []\nfor sentence in tqdm(train['review_text_cleaned']) :\n    cleaned_sentence = [words_dict[word] if word in list(words_dict.keys()) else word for word in sentence.split()]\n    list_sentence_train.append(' '.join(cleaned_sentence))\ntrain['review_text_cleaned'] = list_sentence_train\n\n\nlist_sentence_test = []\nfor sentence in tqdm(test['review_text_cleaned']) :\n    cleaned_sentence = [words_dict[word] if word in list(words_dict.keys()) else word for word in sentence.split()]\n    list_sentence_test.append(' '.join(cleaned_sentence))\ntest['review_text_cleaned'] = list_sentence_test\n\n#removing indonesian stopwords\ntrain['review_text_cleaned_nostopwords'] = remove_indonesian_stop(train, 'review_text_cleaned')\ntest['review_text_cleaned_nostopwords'] = remove_indonesian_stop(test, 'review_text_cleaned')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA"},{"metadata":{},"cell_type":"markdown","source":"#### imbalanced class plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"red_color = '#F42E56'\ngreen_color = '#31B057'\n\nplt.figure(figsize = [10, 5], dpi = 90)\nsns.set_style('whitegrid')\nsns.countplot(train['category'],\n              palette = ['#F42E56', '#31B057'])\nplt.xticks([0, 1], ['Negative', 'Positive'])\nfor index, value in enumerate([train[train['category'] == 0].shape[0], train[train['category'] == 1].shape[0]]) :\n    plt.text(index-.07, value, '%.3f'%(value*100/train.shape[0]) + '%')\nplt.title('Class Distribution Plot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### len_string_initial_plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = [15, 5], dpi = 90)\nsns.distplot(train[train['category'] == 0]['len_string_initial'], \n             color = '#F42E56', \n             label = 'negative; mean = {mean}'.format(mean = '%.0f'%train[train['category'] == 0]['len_string_initial'].mean()))\nsns.distplot(train[train['category'] == 1]['len_string_initial'], \n             color = '#31B057', \n             label = 'positive; mean = {mean}'.format(mean = '%.0f'%train[train['category'] == 1]['len_string_initial'].mean()))\nplt.title('Distplot Comparison Based On Length of Characters')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### num_of_words plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = [15, 5], dpi = 90)\nsns.distplot(train[train['category'] == 0]['num_of_words'], \n             color = '#F42E56', \n             label = 'negative; mean = {mean}'.format(mean = '%.0f'%train[train['category'] == 0]['num_of_words'].mean()))\nsns.distplot(train[train['category'] == 1]['num_of_words'], \n             color = '#31B057', \n             label = 'positive; mean = {mean}'.format(mean = '%.0f'%train[train['category'] == 1]['num_of_words'].mean()))\nplt.title('Distplot Comparison Based on Number of Words')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### num_unique_char plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = [15, 5], dpi = 90)\nsns.distplot(train[train['category'] == 0]['num_unique_char'], \n             color = '#F42E56', \n             label = 'negative; mean = {mean}'.format(mean = '%.0f'%train[train['category'] == 0]['num_unique_char'].mean()))\nsns.distplot(train[train['category'] == 1]['num_unique_char'], \n             color = '#31B057', \n             label = 'positive; mean = {mean}'.format(mean = '%.0f'%train[train['category'] == 1]['num_unique_char'].mean()))\nplt.title('Distplot Compariosn Based on Unique Characters')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Negative Unigram"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = [15, 15])\n\nax1 = fig.add_subplot(1,2,1)\n#class_distribution(\"colname1\", ax=ax1)\nplot_unique_word_count(create_corpus(train[train['category'] == 0], 'review_text_cleaned'), \n                       15, \n                       18, \n                       0, \n                       50,\n                       'Negative Sentiment Unigram (with all stopwords) \\n',\n                       '#F42E56', ax = ax1)\n\nax2 = fig.add_subplot(1,2,2)\nplot_unique_word_count(create_corpus(train[train['category'] == 0], 'review_text_cleaned_nostopwords'), \n                       5, \n                       5, \n                       0, \n                       50,\n                       'Negative Sentiment Unigram (keeping some stopwords) \\n',\n                       '#F42E56', ax = ax2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"stopwords like 'tidak' or the equivalent word in english 'not' are important because the meaning of a sentence could be misleading without their occurence. for example, 'not good' becomes 'good' with stopwords removal. therefore some stopwords are kept for the analysis."},{"metadata":{},"cell_type":"markdown","source":"#### Positive Unigram"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = [15, 15])\n\nax1 = fig.add_subplot(1,2,1)\n#class_distribution(\"colname1\", ax=ax1)\nplot_unique_word_count(create_corpus(train[train['category'] == 1], 'review_text_cleaned'), \n                       15, \n                       18, \n                       0, \n                       50,\n                       'Positive Sentiment Unigram (keeping some stopwords) \\n',\n                       '#31B057', ax = ax1)\n\nax2 = fig.add_subplot(1,2,2)\nplot_unique_word_count(create_corpus(train[train['category'] == 1], 'review_text_cleaned_nostopwords'), \n                       5, \n                       5, \n                       0, \n                       50,\n                       'Positive Sentiment Unigram (keeping some stopwords) \\n',\n                       '#31B057', ax = ax2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Negative and Positive Bigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ndef get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nfig = plt.figure(figsize=(18,18))\ntop_tweet_bigrams=get_top_tweet_bigrams(train[train['category'] == 0]['review_text_cleaned_nostopwords'])[:50]\nx,y=map(list,zip(*top_tweet_bigrams))\nfig.add_subplot(1, 2, 1)\nsns.barplot(x=y,y=x, color = red_color)\nplt.title('Negative Sentiment Bigrams \\n')\nindex = 0\nfor i in top_tweet_bigrams :\n    plt.text(i[1]+.5, index+.2, i[1])\n    index+=1\n    \ntop_tweet_bigrams=get_top_tweet_bigrams(train[train['category'] == 1]['review_text_cleaned_nostopwords'])[:50]\nx,y=map(list,zip(*top_tweet_bigrams))\nfig.add_subplot(1, 2, 2)\nsns.barplot(x=y,y=x, color = green_color)\nplt.title('Positive Sentiment Bigrams \\n')\nindex = 0\nfor i in top_tweet_bigrams :\n    plt.text(i[1]+.5, index+.2, i[1])\n    index+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Negative and Positive Trigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\ndef get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nfig = plt.figure(figsize=(17,18))\ntop_tweet_bigrams=get_top_tweet_bigrams(train[train['category'] == 0]['review_text_cleaned_nostopwords'])[:50]\nx,y=map(list,zip(*top_tweet_bigrams))\nfig.add_subplot(1, 2, 1)\nsns.barplot(x=y,y=x, color = red_color)\nplt.title('Negative Sentiment Trigrams \\n')\nindex = 0\nfor i in top_tweet_bigrams :\n    plt.text(i[1]-30, index+.2, i[1], color = 'white')\n    index+=1\n    \ntop_tweet_bigrams=get_top_tweet_bigrams(train[train['category'] == 1]['review_text_cleaned_nostopwords'])[:50]\nx,y=map(list,zip(*top_tweet_bigrams))\nfig.add_subplot(1, 2, 2)\nsns.barplot(x=y,y=x, color = green_color)\nplt.title('Positive Sentiment Trigrams \\n')\nindex = 0\nfor i in top_tweet_bigrams :\n    plt.text(i[1]-1, index+.2, i[1], color = 'white')\n    index+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Negative and Positive 4-grams"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\ndef get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(4, 4)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nfig = plt.figure(figsize=(19,18))\ntop_tweet_bigrams=get_top_tweet_bigrams(train[train['category'] == 0]['review_text_cleaned_nostopwords'])[:50]\nx,y=map(list,zip(*top_tweet_bigrams))\nfig.add_subplot(1, 2, 1)\nsns.barplot(x=y,y=x, color = red_color)\n#ax.grid(False)\nplt.title('Negative Sentiment 4-grams \\n')\nindex = 0\nfor i in top_tweet_bigrams :\n    plt.text(i[1]-13, index+.2, i[1], color = 'white')\n    index+=1\n    \ntop_tweet_bigrams=get_top_tweet_bigrams(train[train['category'] == 1]['review_text_cleaned_nostopwords'])[:50]\nx,y=map(list,zip(*top_tweet_bigrams))\nfig.add_subplot(1, 2, 2)\nsns.barplot(x=y,y=x, color = green_color)\nplt.title('Positive Sentiment 4-grams \\n')\nindex = 0\nfor i in top_tweet_bigrams :\n    plt.text(i[1]-.7, index+.2, i[1], color = 'white')\n    index+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from the previous n-grams plots, it can be concluded that comfort and cleanliness are 2 main factors that gives positive experience to customers and that's why hotels should prioritize these factors.\n\non the other side, many customers that have bad experience mcomplained about the unavailability of hot water facility, broken air conditioner, and dirty "},{"metadata":{},"cell_type":"markdown","source":"# indoBERT"},{"metadata":{},"cell_type":"markdown","source":"Functions and classes required for the modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport torch\nimport re\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\n#####\n# Document Sentiment\n#####\nclass DocumentSentimentDataset(Dataset):\n    # Static constant variable\n    LABEL2INDEX = {'positive': 1, 'negative': 0}\n    INDEX2LABEL = {1: 'positive', 0: 'negative'}\n    NUM_LABELS = 2\n    \n    def load_dataset(self, path): \n        df = pd.read_csv(path)\n        return df\n    \n    def __init__(self, dataset_path, tokenizer, no_special_token=False, *args, **kwargs):\n        self.data = self.load_dataset(dataset_path)\n        self.tokenizer = tokenizer\n        self.no_special_token = no_special_token\n    \n    def __getitem__(self, index):\n        data = self.data.loc[index,:]\n        text, sentiment = data['review_text'], data['category']\n        subwords = self.tokenizer.encode(text, add_special_tokens=not self.no_special_token)\n        return np.array(subwords), np.array(sentiment), data['review_text']\n    \n    def __len__(self):\n        return len(self.data)    \n        \nclass DocumentSentimentDataLoader(DataLoader):\n    def __init__(self, max_seq_len=512, *args, **kwargs):\n        super(DocumentSentimentDataLoader, self).__init__(*args, **kwargs)\n        self.collate_fn = self._collate_fn\n        self.max_seq_len = max_seq_len\n        \n    def _collate_fn(self, batch):\n        batch_size = len(batch)\n        max_seq_len = max(map(lambda x: len(x[0]), batch))\n        max_seq_len = min(self.max_seq_len, max_seq_len)\n        \n        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n        sentiment_batch = np.zeros((batch_size, 1), dtype=np.int64)\n        \n        seq_list = []\n        for i, (subwords, sentiment, raw_seq) in enumerate(batch):\n            subwords = subwords[:max_seq_len]\n            subword_batch[i,:len(subwords)] = subwords\n            mask_batch[i,:len(subwords)] = 1\n            sentiment_batch[i,0] = sentiment\n            \n            seq_list.append(raw_seq)\n            \n        return subword_batch, mask_batch, sentiment_batch, seq_list\n    \n# Forward function for sequence classification\ndef forward_sequence_classification(model, batch_data, i2w, is_test=False, device='cpu', **kwargs):\n    # Unpack batch data\n    if len(batch_data) == 3:\n        (subword_batch, mask_batch, label_batch) = batch_data\n        token_type_batch = None\n    elif len(batch_data) == 4:\n        (subword_batch, mask_batch, token_type_batch, label_batch) = batch_data\n    \n    # Prepare input & label\n    subword_batch = torch.LongTensor(subword_batch)\n    mask_batch = torch.FloatTensor(mask_batch)\n    token_type_batch = torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n    label_batch = torch.LongTensor(label_batch)\n            \n    if device == \"cuda\":\n        subword_batch = subword_batch.cuda()\n        mask_batch = mask_batch.cuda()\n        token_type_batch = token_type_batch.cuda() if token_type_batch is not None else None\n        label_batch = label_batch.cuda()\n\n    # Forward model\n    outputs = model(subword_batch, attention_mask=mask_batch, token_type_ids=token_type_batch, labels=label_batch)\n    loss, logits = outputs[:2]\n    \n    # generate prediction & label list\n    list_hyp = []\n    list_label = []\n    hyp = torch.topk(logits, 1)[1]\n    for j in range(len(hyp)):\n        list_hyp.append(i2w[hyp[j].item()])\n        list_label.append(i2w[label_batch[j][0].item()])\n        \n    return loss, list_hyp, list_label\n\n\ndef document_sentiment_metrics_fn(list_hyp, list_label):\n    metrics = {}\n    metrics[\"ACC\"] = accuracy_score(list_label, list_hyp)\n    metrics[\"F1\"] = f1_score(list_label, list_hyp, average='macro')\n    metrics[\"REC\"] = recall_score(list_label, list_hyp, average='macro')\n    metrics[\"PRE\"] = precision_score(list_label, list_hyp, average='macro')\n    return metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modelling Initiation"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(train[\"review_text\"], \n                                                    train['category'], train.index, stratify = train[\"category\"],\n                                                    test_size=0.25, random_state=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([X_train,y_train],axis=1).reset_index(drop=True).to_csv(\"train_1.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([train[[\"review_text\"]],train[['category']]], axis=1).reset_index(drop=True).to_csv(\"all_1.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([X_test,y_test],axis=1).reset_index(drop=True).to_csv(\"validation_1.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport torch\nfrom torch import optim\nimport torch.nn.functional as F\n\nfrom transformers import BertForSequenceClassification, BertConfig, BertTokenizer\nfrom nltk.tokenize import TweetTokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    \ndef count_param(module, trainable=False):\n    if trainable:\n        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n    else:\n        return sum(p.numel() for p in module.parameters())\n    \ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef metrics_to_string(metric_dict):\n    string_list = []\n    for key, value in metric_dict.items():\n        string_list.append('{}:{:.4f}'.format(key, value))\n    return ' '.join(string_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_seed(1352)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Tokenizer and Config\ntokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-large-p2\")\nconfig = BertConfig.from_pretrained(\"indobenchmark/indobert-large-p2\")\nconfig.num_labels = 2\n\n# Instantiate model\nmodel = BertForSequenceClassification.from_pretrained(\"indobenchmark/indobert-large-p2\", config=config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_param(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2i = {\"negative\" : 0, \"positive\" : 1}\ni2w = {0:\"negative\", 1:\"positive\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=2e-6)\nmodel = model.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset_path = './train_1.csv'\nvalid_dataset_path = \"./validation_1.csv\"\n\ntrain_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\nvalid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n\ntrain_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=16, num_workers=8, shuffle=True)  \nvalid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=16, num_workers=8, shuffle=False)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss_plot = []\nvalid_loss_plot = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train\nn_epochs = 4\nfor epoch in range(n_epochs):\n    model.train()\n    torch.set_grad_enabled(True)\n \n    total_train_loss = 0\n    list_hyp, list_label = [], []\n\n    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n    for i, batch_data in enumerate(train_pbar):\n        # Forward model\n        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], \n                                                                       i2w=i2w, device='cuda')\n\n        # Update model\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        tr_loss = loss.item()\n        total_train_loss = total_train_loss + tr_loss\n\n        # Calculate metrics\n        list_hyp += batch_hyp\n        list_label += batch_label\n\n        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n            total_train_loss/(i+1), get_lr(optimizer)))\n\n    # Calculate train metric\n    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n\n    # Evaluate on validation\n    model.eval()\n    torch.set_grad_enabled(False)\n    \n    total_loss, total_correct, total_labels = 0, 0, 0\n    list_hyp, list_label = [], []\n\n    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n    for i, batch_data in enumerate(pbar):\n        batch_seq = batch_data[-1]        \n        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], \n                                                                       i2w=i2w, device='cuda')\n        \n        # Calculate total loss\n        valid_loss = loss.item()\n        total_loss = total_loss + valid_loss\n\n        # Calculate evaluation metrics\n        list_hyp += batch_hyp\n        list_label += batch_label\n        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n\n        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), \n                                                           metrics_to_string(metrics)))\n        \n    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n        total_loss/(i+1), metrics_to_string(metrics)))\n    \n    #Tracking of Loss\n    train_loss_plot.append(total_train_loss/len(train_pbar))\n    valid_loss_plot.append(total_loss/len(pbar))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,4))\nplt.title(\"Loss Function\")\nplt.plot(train_loss_plot, label=\"train\")\nplt.plot(valid_loss_plot, label=\"validation\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss Score\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=[]\npred_proba=[]\nfor i in tqdm(range(len(test)), leave=True):\n    text = test[\"review_text\"][i]\n    subwords = tokenizer.encode(text)\n    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n\n    logits = model(subwords)[0]\n    label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n    pred.append(label)\n    proba = F.softmax(logits, dim=-1).squeeze()[label] * 100\n    pred_proba.append(proba.item())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test[[\"review_id\"]]\nsubmission[\"category\"] = pred\nsubmission[\"category_probs\"] = pred_proba\nlist_to_change = submission[(submission[\"category\"] == 1) & (submission[\"category_probs\"] < 50)].index.tolist()\nsubmission.iloc[list_to_change,1] = 0\nsubmission.drop(\"category_probs\",axis=1,inplace=True)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"category\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission_final.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dict format {'models' : [accuracy : f1]}\nval_dict = {'indoBERT base (preprocessing)' : [0.9555, 0.9041],\n                   'indoBERT base (raw)' : [0.9634, 0.9170],\n                   'indoBERT large (preprocessing)' : [0.9587, 0.9055],\n                   'indoBERT large (raw)' : [0.9631, 0.9200],\n                   'LinearSVC TF-IDF + preprocessing' : [0.942085, 0.867323],\n                   'indoBERT base phase 1 pretrained' : [0.16720, 0.16045603467157965]}\n\nval_df = pd.DataFrame(val_dict).transpose()\nval_df.columns = ['accuracy', 'f1']\nplt.figure(figsize = [15 ,6])\nax = sns.barplot(data = val_df.sort_values(by = 'f1', ascending = False),\n            x = 'f1',\n            y = val_df.sort_values(by = 'f1', ascending = False).index,\n            palette = [green_color, red_color, red_color, red_color, red_color, red_color])\nax.set_yticklabels(ax.get_yticklabels())\nax.set_xlabel('Macro F1-score')\nax.set_xticks([])\nfor index, value in enumerate(val_df.sort_values(by = 'f1', ascending = False)['f1']) :\n    plt.text(value, index, '%.3f'%value)\nplt.title('Validation Phase Comparison Between Models \\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the previous plot is a performace-comparison plot between methods. the best model is obtained using large indoBERT using raw data (without preprocessing) and using fine tuning (without pre-trained weights). these results are obtained from other kernel.\n\ntherefore fine tuning indoBERT is used to build a classifier to classify negative and positive sentiment based on hotel review data with 0.92 macro f1-score obtained in the validation phase."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}